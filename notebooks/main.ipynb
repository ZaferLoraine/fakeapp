{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e7fde27-a854-4e41-b504-cbcd5b8ef954",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/01 03:03:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Relatório para categoria: MEN'S CLOTHING ===\n",
      "Q1: 15.99 | Q3: 55.99 | IQR: 40.00\n",
      "Limites: [-44.01, 115.99]\n",
      "+---+-----+-----+--------+\n",
      "|id |title|price|category|\n",
      "+---+-----+-----+--------+\n",
      "+---+-----+-----+--------+\n",
      "\n",
      "\n",
      "=== Relatório para categoria: JEWELERY ===\n",
      "Q1: 9.99 | Q3: 168.00 | IQR: 158.01\n",
      "Limites: [-227.02, 405.01]\n",
      "+---+---------------------------------------------------------------------------+-----+--------+\n",
      "|id |title                                                                      |price|category|\n",
      "+---+---------------------------------------------------------------------------+-----+--------+\n",
      "|5  |John Hardy Women's Legends Naga Gold & Silver Dragon Station Chain Bracelet|695.0|jewelery|\n",
      "+---+---------------------------------------------------------------------------+-----+--------+\n",
      "\n",
      "\n",
      "=== Relatório para categoria: ELECTRONICS ===\n",
      "Q1: 109.00 | Q3: 599.00 | IQR: 490.00\n",
      "Limites: [-626.00, 1334.00]\n",
      "+---+-----+-----+--------+\n",
      "|id |title|price|category|\n",
      "+---+-----+-----+--------+\n",
      "+---+-----+-----+--------+\n",
      "\n",
      "\n",
      "=== Relatório para categoria: WOMEN'S CLOTHING ===\n",
      "Q1: 9.85 | Q3: 39.99 | IQR: 30.14\n",
      "Limites: [-35.36, 85.20]\n",
      "+---+-----+-----+--------+\n",
      "|id |title|price|category|\n",
      "+---+-----+-----+--------+\n",
      "+---+-----+-----+--------+\n",
      "\n",
      "Salvando parte 1...\n",
      "Partes salvas com sucesso!\n"
     ]
    }
   ],
   "source": [
    "#bibliotecas importadas\n",
    "from processamento import FakeStoreDataProcessor  # Importa a classe responsável por processar os dados\n",
    "from sqlalchemy import create_engine              # Utilizada para conectar com o PostgreSQL via SQLAlchemy\n",
    "from pyspark.sql import DataFrame\n",
    "import traceback                                  # Para imprimir rastreamento de erros detalhados\n",
    "\n",
    "def main():\n",
    "    # Instancia o processador de dados\n",
    "    processor = FakeStoreDataProcessor()\n",
    "\n",
    "    # Etapas do pipeline:\n",
    "    # 1. Obter dados brutos da API\n",
    "    df_raw = processor.processar_dados()\n",
    "\n",
    "    # 2. Limpar dados (remover nulos e outliers)\n",
    "    df_limpo = processor.limpar_dados(df_raw)\n",
    "\n",
    "    # 3. Filtrar apenas produtos com preço >= 100 e avaliação >= 3.5\n",
    "    df_filtrado = processor.filtrar_preco_avaliacao(df_limpo)\n",
    "\n",
    "    # 4. Gerar resumo por categoria (preço médio e avaliação média)\n",
    "    df_resumo = processor.resumir_por_categoria(df_filtrado)\n",
    "\n",
    "    # 5. Dividir o resumo em partes menores (5 registros por parte)\n",
    "    partes = processor.dividir_em_partes(df_resumo, registros_por_parte=5)\n",
    "\n",
    "    # Criação do engine para conexão com o PostgreSQL\n",
    "    engine = create_engine(\"postgresql://admin:admin@postgres:5432/spark_db\")\n",
    "    connection = engine.connect()\n",
    "    trans = connection.begin()  # Início de uma transação para controle de falhas\n",
    "\n",
    "    try:\n",
    "        # Loop para salvar cada parte do DataFrame no PostgreSQL\n",
    "        for i, parte in enumerate(partes):\n",
    "            print(f\"Salvando parte {i+1}...\")\n",
    "\n",
    "            parte.write \\\n",
    "                .mode(\"append\") \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"url\", \"jdbc:postgresql://postgres:5432/spark_db\") \\\n",
    "                .option(\"dbtable\", \"resumo_categorias\") \\\n",
    "                .option(\"user\", \"admin\") \\\n",
    "                .option(\"password\", \"admin\") \\\n",
    "                .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "                .option(\"batchsize\", 1000) \\\n",
    "                .option(\"numPartitions\", 1) \\\n",
    "                .save()\n",
    "\n",
    "        # Confirma a transação\n",
    "        trans.commit()\n",
    "        print(\"Partes salvas com sucesso!\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Em caso de erro, reverte a transação\n",
    "        print(\"Erro. Realizando rollback...\")\n",
    "        trans.rollback()\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    finally:\n",
    "        # Fecha a conexão com o banco\n",
    "        connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0351c43e-7ac7-4aa9-a786-40f57069c479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
